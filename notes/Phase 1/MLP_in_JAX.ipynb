{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb57922-8ca4-43eb-8966-7ca14418791b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp\n",
    "from jax import random, grad, jit, vmap, tree_util, nn\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7b05c8-37d2-4cff-a265-35989e7598f2",
   "metadata": {},
   "source": [
    "Since JAX follows the pure functional approach, so in order to track model's parameters we need to build a pytree containing parameters(weights and biases) for each layer which may seem unusual at first but that's the pure JAX way of tracking params instead of having them as attributes of objects of each layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bd4063-879d-43fb-ad2e-1117dcda48da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(key, input_dim, layer_dims):\n",
    "    \"\"\"Initializes the parameters for the Multi Layer Perceptron\"\"\"\n",
    "\n",
    "    params = []\n",
    "    keys = random.split(key, len(layer_dims))\n",
    "    for in_dim, out_dim, key in zip([input_dim]+layer_dims[:-1],layer_dims,keys):\n",
    "        w_key, b_key = random.split(key)\n",
    "    \n",
    "        w = nn.initializers.he_normal()(key, (in_dim, out_dim))\n",
    "        b = jnp.zeros(out_dim)\n",
    "        params.append({'w':w,'b':b})\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8756d46a-96a4-4ca8-8076-5356c7fb9d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  apply_model(params, inputs):\n",
    "    \"\"\"Forward Pass of the model\"\"\"\n",
    "    \n",
    "    x = inputs \n",
    "  \n",
    "    for layer in params[:-1]:\n",
    "        x = x @ layer['w'] + layer['b']\n",
    "        x = jax.nn.relu(x)\n",
    "\n",
    "    final_layer = params[-1]\n",
    "    \n",
    "    x = x @ final_layer['w'] + final_layer['b']\n",
    "    # return jax.nn.log_softmax(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a930f582-ebed-4533-a1d2-9e35893293b6",
   "metadata": {},
   "source": [
    "Here we did not applied the `softmax` activation in the output layer because in the `loss_fn` we are calculating derivative of cross_entropy which gets cancelled out pretty nicely when paired with softmax resulting in much efficient calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07726c6-1403-490b-8c89-a0dd648d3772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, inputs, targets):\n",
    "    \"\"\"Calculates the loss for the model\"\"\"\n",
    "    logits = apply_model(params, inputs)\n",
    "\n",
    "    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits = logits, labels = targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "925070e3-7774-4f8d-82ee-aa7898ad4e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_fn(params, grads, learning_rate):\n",
    "    \"\"\"Updates the parameters using SGD update Formula\"\"\"    \n",
    "    return tree_util.tree_map(\n",
    "        lambda p,g : p - learning_rate * g,\n",
    "        params,\n",
    "        grads\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b3207df-97fd-43b4-9fb3-af3c5a7e461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def train_step(params, inputs, targets, learning_rate = 0.001):\n",
    "    \n",
    "    grads = grad(loss_fn)(params, inputs, targets)\n",
    "    return update_fn(params, grads, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddaeb15-0431-46a0-8e61-b8326a97251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def predict(params, inputs):\n",
    "    \n",
    "    return jnp.argmax(apply_model(params, inputs),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4581fb64-3c6e-4ab1-8a04-3686ea450763",
   "metadata": {},
   "source": [
    "Note :- In the training step the final layer's `softmax` activation was paired with `loss_fn` but in the predict we are not using softmax at all this is because applying softmax activation here won't change the model's output, the bigger values will get higher probability and smaller will get lower, so to make inference more efficient we didn't applied it here. However if needed to build the `confidence interval` we would have applied the `softmax` activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e811ee4-9103-4dfa-985d-5558fda280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cecfc970-6709-431d-93f4-d24060afe1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sns.load_dataset(\"penguins\")\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "non_numerical_cols = ['island','sex']\n",
    "\n",
    "for i in non_numerical_cols:\n",
    "    data[i]=le.fit_transform(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4208bb14-66f3-4f0a-93d2-bbe88131b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('species',axis=1).values\n",
    "y = le.fit_transform(data['species'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e4f2206-5c60-4a31-85dd-10cc0813905e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training...\n",
      "Epoch 1, Loss = 1.6930781602859497 , Time = 0.11561083793640137s\n",
      "Epoch 20, Loss = 1.2324516773223877 , Time = 0.0s\n",
      "Epoch 40, Loss = 0.9929127097129822 , Time = 0.0s\n",
      "Epoch 60, Loss = 0.8408139944076538 , Time = 0.0s\n",
      "Epoch 80, Loss = 0.7315548062324524 , Time = 0.0s\n",
      "Epoch 100, Loss = 0.6493933200836182 , Time = 0.0s\n",
      "Epoch 120, Loss = 0.5852449536323547 , Time = 0.0s\n",
      "Epoch 140, Loss = 0.5339083075523376 , Time = 0.0s\n",
      "Epoch 160, Loss = 0.4920057952404022 , Time = 0.0s\n",
      "Epoch 180, Loss = 0.4572613835334778 , Time = 0.0s\n",
      "Epoch 200, Loss = 0.4281741976737976 , Time = 0.0s\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Started training...\")\n",
    "key = random.PRNGKey(0)\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "params = init_model(key, input_dim = 6, layer_dims=[10,3])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start=time.time()\n",
    "    params = train_step(params, X_train, y_train, learning_rate)\n",
    "    end=time.time()\n",
    "\n",
    "    if(epoch+1) % 20 == 0 or (epoch+1)==1:\n",
    "        current_loss = loss_fn(params, X_train, y_train)\n",
    "        print(f\"Epoch {epoch+1}, Loss = {current_loss} , Time = {end-start}s\")\n",
    "\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402878ec-663d-4658-9a51-97e3cf3892c0",
   "metadata": {},
   "source": [
    "As we can see above that the first run(epoch) of the training step took longer than subsequent calls because of `JIT compilation` of the `train_step` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb801f84-0fa1-43b1-8da5-a74ac17f596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing...\n",
      "Accuracy: 0.8656716417910447\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing...\")\n",
    "y_pred = predict(params, X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jax-env)",
   "language": "python",
   "name": "jax.env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
